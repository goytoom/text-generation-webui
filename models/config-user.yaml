Meta-Llama-3-8B-Instruct-Q8_0.gguf$:
  loader: llama.cpp
  cpu: false
  cache_8bit: false
  cache_4bit: false
  threads: 0
  threads_batch: 0
  n_batch: 512
  no_mmap: false
  mlock: false
  no_mul_mat_q: false
  n_gpu_layers: 256
  tensor_split: ''
  n_ctx: 8192
  compress_pos_emb: 1
  rope_freq_base: 500000
  numa: false
  no_offload_kqv: false
  row_split: false
  tensorcores: false
  flash_attn: false
  streaming_llm: false
  attention_sink_size: 5
